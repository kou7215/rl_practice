{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsaの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0 = np.array([[np.nan, 1, 1, np.nan],\n",
    "                    [np.nan, 1, np.nan, 1],\n",
    "                    [np.nan, np.nan, 1, 1],\n",
    "                    [1, 1, 1, np.nan],\n",
    "                    [np.nan, np.nan, 1, 1],\n",
    "                    [1, np.nan, np.nan, np.nan],\n",
    "                    [1, np.nan, np.nan, np.nan],\n",
    "                    [1, 1, np.nan, np.nan]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       nan, 0.71721336, 0.18079309,        nan],\n",
       "       [       nan, 0.12930881,        nan, 0.28918835],\n",
       "       [       nan,        nan, 0.29966057, 0.54963378],\n",
       "       [0.202665  , 0.14937788, 0.24500776,        nan],\n",
       "       [       nan,        nan, 0.67120503, 0.72994102],\n",
       "       [0.83440524,        nan,        nan,        nan],\n",
       "       [0.92664781,        nan,        nan,        nan],\n",
       "       [0.92010901, 0.98461753,        nan,        nan]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a, b] = theta_0.shape\n",
    "Q = np.random.rand(a, b) * theta_0\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方策パラメータtheta_0をランダム方策piに変換する関数の定義\n",
    "\n",
    "単純に割合を計算するだけ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.5       , 0.5       , 0.        ],\n",
       "       [0.        , 0.5       , 0.        , 0.5       ],\n",
       "       [0.        , 0.        , 0.5       , 0.5       ],\n",
       "       [0.33333333, 0.33333333, 0.33333333, 0.        ],\n",
       "       [0.        , 0.        , 0.5       , 0.5       ],\n",
       "       [1.        , 0.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.5       , 0.5       , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_convert_into_pi_from_theta(theta):\n",
    "    m, n = theta.shape\n",
    "    pi = np.zeros((m, n))\n",
    "    for i in range(0, m):\n",
    "        pi[i, :] = theta[i, :] / np.nansum(theta[i, :])\n",
    "    \n",
    "    pi = np.nan_to_num(pi)\n",
    "    \n",
    "    return pi\n",
    "\n",
    "pi_0 = simple_convert_into_pi_from_theta(theta_0)\n",
    "pi_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ε-greedyの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(s, Q, epsilon, pi_0):\n",
    "    direction = ['up', 'right', 'down', 'left']\n",
    "    \n",
    "    # 行動を決める\n",
    "    if np.random.rand() < epsilon:\n",
    "        # epsilon以下なら方策πから行動を決定\n",
    "        # ※方策は改善されないので実質一様分布から選択. \n",
    "        next_direction = np.random.choice(direction, p=pi_0[s, :])\n",
    "    else:\n",
    "        # epsilon以上なら各s,aでの報酬に基づき評価された行動価値関数Qから次のaを選択.\n",
    "        next_direction = direction[np.nanargmax(Q[s, :])]\n",
    "    \n",
    "    if next_direction == 'up':\n",
    "        action = 0\n",
    "    elif next_direction == 'right':\n",
    "        action = 1\n",
    "    elif next_direction == 'down':\n",
    "        action = 2\n",
    "    elif next_direction == 'left':\n",
    "        action = 3\n",
    "    return action\n",
    "\n",
    "def get_s_next(s, a, Q, epsilon, pi_0):\n",
    "    direction = ['up', 'right', 'down', 'left']\n",
    "    next_direction = direction[a]\n",
    "    \n",
    "    # 行動から次の状態を決める\n",
    "    if next_direction == 'up':\n",
    "        s_next = s - 3\n",
    "    elif next_direction == 'right':\n",
    "        s_next = s + 1\n",
    "    elif next_direction == 'down':\n",
    "        s_next = s + 3\n",
    "    elif next_direction == 'left':\n",
    "        s_next = s - 1\n",
    "    return s_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsaによる行動価値関数Qの更新\n",
    "\n",
    "TD誤差を用いて行動価値関数$Q(s, a)$を更新する.\n",
    "$Q(s, a)$の正しい値が求まっていれば、ベルマン方程式を用いて、\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = R_{t+1} + γQ(s_{t+1}, a_{t+1})　\\tag{1}\n",
    "$$\n",
    "\n",
    "として求められるが、学習の途中ではこの値は成立しない。<br>\n",
    "そこで、以下のようにTD誤差を計算して更新を行う.\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + η(R_{t+1} + γQ(S_{t+1}, a_{t+1}) - Q(s_t, a_t)) \\tag{2}\n",
    "$$\n",
    "\n",
    "ベルマン方程式$(1)$は$Q(s, a)$が正しい値であるときに成り立つ式であるので、その誤差を最小化させるようなイメージ.\n",
    "\n",
    "次の行動$a_{next}$における行動価値関数の値を決める際には、過去の行動価値関数あるいはε-greedy方策に基づくのでSarsaは方策オン型アルゴリズムとなる."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa(s, a, r, s_next, a_next, Q, eta, gamma):\n",
    "    if s_next == 8:\n",
    "        # ゴールした場合は次の状態がないので、Q[s_next, a_next] = 0\n",
    "        Q[s, a] = Q[s, a] + eta*(r - Q[s, a])\n",
    "    else:\n",
    "        # 以下、Qの更新にはs_nextと過去の経験/greedy方策で決定したa_nextを用いるので方策オン型.\n",
    "        # Q-learningはa_nextが\n",
    "        Q[s, a] = Q[s, a] + eta*(r + gamma*Q[s_next, a_next] - Q[s, a])\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsaで迷路を解く\n",
    "\n",
    "方策勾配法と異なり、価値反復法では1エピソードごとではなく、1stepごとに更新する.\n",
    "\n",
    "ここでは方策改善はしていない点に注意。あくまで、$s$, $a$を入力とする行動価値関数$Q$のみで次の状態・行動の学習・実行をしている.\n",
    "\n",
    "Sarsaでは行動価値関数$Q$の更新に、ε-greedyに基づいて選択された$a_t$, $a_{t+1}$両方が考慮される.\n",
    "\n",
    "※Q-learningでは$a_t$はε-greedy, $a_{t+1}$もε-greedyで選択されるが, $Q$の更新に$a_{t+1}$は使わず, $\\max_{a}Q(s_{t+1}, a)$とする.<br>\n",
    "　あくまで$a_t \\leftarrow a_{t+1}$としてアップデートするためだけにε-greedyするイミッジ. 更新式には直接入らない.<br>\n",
    "　Sarsaでは$Q$更新時にε-greedyに基づいて選択された$a_{t+1}$が考慮されているため、迷路で通れない場所のゆらぎを加味して評価されるが, <br>\n",
    "　Q-learningでは常に評価が正しくできていると仮定して次の評価値へ更新するため, 通れないところのギリギリを攻めたルート選択をするようになる. ただしその分収束は早い."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi):\n",
    "    s = 0\n",
    "    a = a_next = get_action(s, Q, epsilon, pi)\n",
    "    s_a_history = [[0, np.nan]]\n",
    "    \n",
    "    # 1episode終わったら状態・行動の履歴を返す.\n",
    "    while(1):\n",
    "        a = a_next\n",
    "        s_a_history[-1][1] = a\n",
    "        \n",
    "        s_next = get_s_next(s, a, Q, epsilon, pi)\n",
    "        \n",
    "        s_a_history.append([s_next, np.nan])\n",
    "        \n",
    "        if s_next == 8:\n",
    "            r = 1\n",
    "            a_next = np.nan\n",
    "        else:\n",
    "            r = 0\n",
    "            a_next = get_action(s_next, Q, epsilon, pi)\n",
    "            \n",
    "        # 1 stepごとに(各s, aごとに)Qは更新される.\n",
    "        Q = Sarsa(s, a, r, s_next, a_next, Q, eta, gamma)\n",
    "        \n",
    "        if s_next == 8:\n",
    "            break\n",
    "        else:\n",
    "            s = s_next\n",
    "    \n",
    "    return [s_a_history, Q]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実際にSarsaで迷路問題を解いてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  1\n",
      "1.3053232947919642\n",
      "step in the end of episode :  256\n",
      "episode :  2\n",
      "0.9129733752584758\n",
      "step in the end of episode :  276\n",
      "episode :  3\n",
      "0.055595825460599296\n",
      "step in the end of episode :  4\n",
      "episode :  4\n",
      "0.054669750133107486\n",
      "step in the end of episode :  4\n",
      "episode :  5\n",
      "0.053515777975227385\n",
      "step in the end of episode :  4\n",
      "episode :  6\n",
      "0.052175372789982355\n",
      "step in the end of episode :  4\n",
      "episode :  7\n",
      "0.05068523138777409\n",
      "step in the end of episode :  4\n",
      "episode :  8\n",
      "0.04907770675010287\n",
      "step in the end of episode :  4\n",
      "episode :  9\n",
      "0.04738120488183106\n",
      "step in the end of episode :  4\n",
      "episode :  10\n",
      "0.045620555408332875\n",
      "step in the end of episode :  4\n",
      "episode :  11\n",
      "0.043817356314081346\n",
      "step in the end of episode :  4\n",
      "episode :  12\n",
      "0.041990293482871366\n",
      "step in the end of episode :  4\n",
      "episode :  13\n",
      "0.040155435899631775\n",
      "step in the end of episode :  4\n",
      "episode :  14\n",
      "0.038326507520372866\n",
      "step in the end of episode :  4\n",
      "episode :  15\n",
      "0.03651513691951608\n",
      "step in the end of episode :  4\n",
      "episode :  16\n",
      "0.03473108589046031\n",
      "step in the end of episode :  4\n",
      "episode :  17\n",
      "0.0329824582124677\n",
      "step in the end of episode :  4\n",
      "episode :  18\n",
      "0.03127588981043905\n",
      "step in the end of episode :  4\n",
      "episode :  19\n",
      "0.029616721528675005\n",
      "step in the end of episode :  4\n",
      "episode :  20\n",
      "0.028009155719321388\n",
      "step in the end of episode :  4\n",
      "episode :  21\n",
      "0.026456397814236476\n",
      "step in the end of episode :  4\n",
      "episode :  22\n",
      "0.024960784008336623\n",
      "step in the end of episode :  4\n",
      "episode :  23\n",
      "0.02352389613540018\n",
      "step in the end of episode :  4\n",
      "episode :  24\n",
      "0.022146664765791302\n",
      "step in the end of episode :  4\n",
      "episode :  25\n",
      "0.020829461501193025\n",
      "step in the end of episode :  4\n",
      "episode :  26\n",
      "0.019572181385506204\n",
      "step in the end of episode :  4\n",
      "episode :  27\n",
      "0.01837431629463604\n",
      "step in the end of episode :  4\n",
      "episode :  28\n",
      "0.01723502011177236\n",
      "step in the end of episode :  4\n",
      "episode :  29\n",
      "0.0161531664396527\n",
      "step in the end of episode :  4\n",
      "episode :  30\n",
      "0.01512739954766451\n",
      "step in the end of episode :  4\n",
      "episode :  31\n",
      "0.01415617919989709\n",
      "step in the end of episode :  4\n",
      "episode :  32\n",
      "0.013237819960673858\n",
      "step in the end of episode :  4\n",
      "episode :  33\n",
      "0.012370525526859133\n",
      "step in the end of episode :  4\n",
      "episode :  34\n",
      "0.011552418591495694\n",
      "step in the end of episode :  4\n",
      "episode :  35\n",
      "0.010781566701121625\n",
      "step in the end of episode :  4\n",
      "episode :  36\n",
      "0.01005600452949118\n",
      "step in the end of episode :  4\n",
      "episode :  37\n",
      "0.009373752953347192\n",
      "step in the end of episode :  4\n",
      "episode :  38\n",
      "0.008732835281312301\n",
      "step in the end of episode :  4\n",
      "episode :  39\n",
      "0.00813129095484788\n",
      "step in the end of episode :  4\n",
      "episode :  40\n",
      "0.007567187010435572\n",
      "step in the end of episode :  4\n",
      "episode :  41\n",
      "0.0070386275646230345\n",
      "step in the end of episode :  4\n",
      "episode :  42\n",
      "0.006543761558193584\n",
      "step in the end of episode :  4\n",
      "episode :  43\n",
      "0.006080788972382956\n",
      "step in the end of episode :  4\n",
      "episode :  44\n",
      "0.005647965708652225\n",
      "step in the end of episode :  4\n",
      "episode :  45\n",
      "0.005243607303910047\n",
      "step in the end of episode :  4\n",
      "episode :  46\n",
      "0.00486609163516416\n",
      "step in the end of episode :  4\n",
      "episode :  47\n",
      "0.00451386075123783\n",
      "step in the end of episode :  4\n",
      "episode :  48\n",
      "0.004185421954312929\n",
      "step in the end of episode :  4\n",
      "episode :  49\n",
      "0.00387934824055447\n",
      "step in the end of episode :  4\n",
      "episode :  50\n",
      "0.0035942781968130166\n",
      "step in the end of episode :  4\n",
      "episode :  51\n",
      "0.003328915439322122\n",
      "step in the end of episode :  4\n",
      "episode :  52\n",
      "0.0030820276702799854\n",
      "step in the end of episode :  4\n",
      "episode :  53\n",
      "0.002852445419181615\n",
      "step in the end of episode :  4\n",
      "episode :  54\n",
      "0.0026390605276275236\n",
      "step in the end of episode :  4\n",
      "episode :  55\n",
      "0.0024408244290331504\n",
      "step in the end of episode :  4\n",
      "episode :  56\n",
      "0.00225674626811323\n",
      "step in the end of episode :  4\n",
      "episode :  57\n",
      "0.002085890899142462\n",
      "step in the end of episode :  4\n",
      "episode :  58\n",
      "0.001927376796767355\n",
      "step in the end of episode :  4\n",
      "episode :  59\n",
      "0.0017803739084595316\n",
      "step in the end of episode :  4\n",
      "episode :  60\n",
      "0.001644101473555315\n",
      "step in the end of episode :  4\n",
      "episode :  61\n",
      "0.0015178258301317138\n",
      "step in the end of episode :  4\n",
      "episode :  62\n",
      "0.0014008582277050818\n",
      "step in the end of episode :  4\n",
      "episode :  63\n",
      "0.0012925526608441595\n",
      "step in the end of episode :  4\n",
      "episode :  64\n",
      "0.0011923037362576716\n",
      "step in the end of episode :  4\n",
      "episode :  65\n",
      "0.0010995445836625706\n",
      "step in the end of episode :  4\n",
      "episode :  66\n",
      "0.0010137448187983455\n",
      "step in the end of episode :  4\n",
      "episode :  67\n",
      "0.0009344085652204237\n",
      "step in the end of episode :  4\n",
      "episode :  68\n",
      "0.0008610725400359787\n",
      "step in the end of episode :  4\n",
      "episode :  69\n",
      "0.0007933042074322882\n",
      "step in the end of episode :  4\n",
      "episode :  70\n",
      "0.0007307000027509947\n",
      "step in the end of episode :  4\n",
      "episode :  71\n",
      "0.0006728836288867335\n",
      "step in the end of episode :  4\n",
      "episode :  72\n",
      "0.0006195044259766913\n",
      "step in the end of episode :  4\n",
      "episode :  73\n",
      "0.0005702358146527642\n",
      "step in the end of episode :  4\n",
      "episode :  74\n",
      "0.0005247738125314649\n",
      "step in the end of episode :  4\n",
      "episode :  75\n",
      "0.0004828356231367792\n",
      "step in the end of episode :  4\n",
      "episode :  76\n",
      "0.0004441582960333923\n",
      "step in the end of episode :  4\n",
      "episode :  77\n",
      "0.00040849745662352444\n",
      "step in the end of episode :  4\n",
      "episode :  78\n",
      "0.00037562610378738537\n",
      "step in the end of episode :  4\n",
      "episode :  79\n",
      "0.0003453334733376501\n",
      "step in the end of episode :  4\n",
      "episode :  80\n",
      "0.0003174239650955979\n",
      "step in the end of episode :  4\n",
      "episode :  81\n",
      "0.0002917161312760985\n",
      "step in the end of episode :  4\n",
      "episode :  82\n",
      "0.00026804172378747104\n",
      "step in the end of episode :  4\n",
      "episode :  83\n",
      "0.00024624479799484345\n",
      "step in the end of episode :  4\n",
      "episode :  84\n",
      "0.0002261808704763224\n",
      "step in the end of episode :  4\n",
      "episode :  85\n",
      "0.0002077161282936224\n",
      "step in the end of episode :  4\n",
      "episode :  86\n",
      "0.00019072668731823228\n",
      "step in the end of episode :  4\n",
      "episode :  87\n",
      "0.000175097897183063\n",
      "step in the end of episode :  4\n",
      "episode :  88\n",
      "0.00016072369047714918\n",
      "step in the end of episode :  4\n",
      "episode :  89\n",
      "0.00014750597385504438\n",
      "step in the end of episode :  4\n",
      "episode :  90\n",
      "0.00013535405879461226\n",
      "step in the end of episode :  4\n",
      "episode :  91\n",
      "0.0001241841298152968\n",
      "step in the end of episode :  4\n",
      "episode :  92\n",
      "0.00011391874803257096\n",
      "step in the end of episode :  4\n",
      "episode :  93\n",
      "0.00010448638801319188\n",
      "step in the end of episode :  4\n",
      "episode :  94\n",
      "9.582100597582688e-05\n",
      "step in the end of episode :  4\n",
      "episode :  95\n",
      "8.786163745999609e-05\n",
      "step in the end of episode :  4\n",
      "episode :  96\n",
      "8.055202267576167e-05\n",
      "step in the end of episode :  4\n",
      "episode :  97\n",
      "7.38402578270847e-05\n",
      "step in the end of episode :  4\n",
      "episode :  98\n",
      "6.767847078603673e-05\n",
      "step in the end of episode :  4\n",
      "episode :  99\n",
      "6.202251957510008e-05\n",
      "step in the end of episode :  4\n",
      "episode :  100\n",
      "5.683171219639238e-05\n",
      "step in the end of episode :  4\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.5\n",
    "v = np.nanmax(Q, axis=1) # 状態ごとに価値の最大値を求める.\n",
    "is_continue = True\n",
    "episode = 1\n",
    "v_losses = []\n",
    "\n",
    "while is_continue:\n",
    "    print('episode : ', episode)\n",
    "    \n",
    "    # epsilonの値を少しずつ小さくする\n",
    "    # -> だんだん経験を重視するようになる.\n",
    "    epsilon /= 2\n",
    "    \n",
    "    # Sarsaで迷路を解き、移動した履歴と更新したQを求める\n",
    "    [s_a_history, Q] = goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi_0)\n",
    "    \n",
    "    # 状態価値の変化\n",
    "    new_v = np.nanmax(Q, axis=1)\n",
    "    v_loss = np.sum(np.abs(new_v - v))\n",
    "    v_losses.append(v_loss)\n",
    "    print(v_loss)\n",
    "    v = new_v\n",
    "    \n",
    "    print('step in the end of episode : ', len(s_a_history) - 1)\n",
    "    \n",
    "    episode += 1\n",
    "    if episode > 100:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAbo0lEQVR4nO3dfZBcV33m8e/Tt1+kmZFfNaSMZCGzCBKFtQkZHLOBDQYWbIeNslXZYPOS4IVSsYWzsKESzELIsmzVQpFsQRaDVuU4Dkti7y64guMymLwuqSUmlllibIhAMTYWstGLLUszeunp7t/+cW+3ekY9o9bLnZbmPJ+qpvvee7r7HNv0M+ece89VRGBmZumqjLoCZmY2Wg4CM7PEOQjMzBLnIDAzS5yDwMwscQ4CM7PEOQjMTkDSY5JeO+p6mJXFQWBmljgHgZlZ4hwEZkOS1JD0CUm7iscnJDWKY6sl3SNpv6SnJf2NpEpx7H2SfijpoKTtkl4z2paYzVUddQXMziEfAK4CXgIE8EXgg8BvAe8FdgKTRdmrgJD0IuAm4GURsUvSeiBb2mqbLc49ArPhvRn4TxGxOyL2AB8G3locmwUuAZ4XEbMR8TeRL+TVBhrARkm1iHgsIv5xJLU3W4CDwGx4zwUe79t+vNgH8HFgB/AVSY9KuhkgInYA7wH+I7Bb0p2SnovZWcRBYDa8XcDz+rbXFfuIiIMR8d6IeD7wL4Ff784FRMQfR8QrivcG8LGlrbbZ4hwEZsO7A/igpElJq4EPAZ8DkPQGSS+QJOAA+ZBQW9KLJL26mFQ+AhwujpmdNRwEZsP7z8A24CHgW8A3in0AG4A/B6aBvwU+HRF/TT4/8FFgL/AU8BzgPyxprc1OQL4xjZlZ2twjMDNLnIPAzCxxDgIzs8Q5CMzMEnfOLTGxevXqWL9+/airYWZ2TnnwwQf3RsTkoGPnXBCsX7+ebdu2jboaZmbnFEmPL3TMQ0NmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWuGSCYPtTB/ndr2zn6ZnmqKtiZnZWSSYIHt0zzX/7yx089eyRUVfFzOyskkwQjDfyi6hnmq0R18TM7OySXBBMH3UQmJn1SyYIVq0oegQOAjOzOZIJgl6P4IiDwMysXzJBMFH30JCZ2SDJBMF4IwNg5mh7xDUxMzu7JBME1azCilrFZw2Zmc2TTBAATDSqHPQcgZnZHEkFwXij6rOGzMzmSSsI6g4CM7P5kgqCiRVVnzVkZjZPWkHQcBCYmc1XWhBIuk3SbkkPL3D8zZIeKh5fk3RFWXXp8hyBmdnxyuwR3A5cs8jx7wM/FxGXAx8BtpZYFwAmGhnTvo7AzGyOalkfHBFflbR+keNf69u8H1hbVl26JtwjMDM7ztkyR/B24EsLHZS0WdI2Sdv27Nlzyl8y3qhyeLZNq9055c8wM1tuRh4Ekq4mD4L3LVQmIrZGxFRETE1OTp7yd0307kng4SEzs66RBoGky4FbgU0Rsa/s7+vdnMbDQ2ZmPSMLAknrgLuAt0bEd5fiOyccBGZmxyltsljSHcCrgNWSdgK/DdQAImIL8CHgYuDTkgBaETFVVn3gWBAcdBCYmfWUedbQDSc4/g7gHWV9/yAeGjIzO97IJ4uX0rF7EjgIzMy6kgqCVY0agC8qMzPrk1QQdHsE00dmR1wTM7OzR2JB4OsIzMzmSyoIGtUK1Yq8AqmZWZ+kgkASEyu83pCZWb+kggDyu5RN+77FZmY9yQWBb05jZjZXckEw3siYaToIzMy6kguCiRU1X0dgZtYnvSBoZL6OwMysT3JBMF6vMuMegZlZT3pB4NtVmpnNkVwQrFpRZabZIiJGXRUzs7NCckEw3qjSCTg86+EhMzNINAgAX1RmZlZILggmuiuQep7AzAxIMgjyexL4zCEzs1xyQTDuHoGZ2RzJBUH3BvYOAjOzXHJB4BvYm5nNVVoQSLpN0m5JDy9wXJJ+T9IOSQ9JemlZdem3yj0CM7M5yuwR3A5cs8jxa4ENxWMz8JkS69LjHoGZ2VylBUFEfBV4epEim4DPRu5+4AJJl5RVn66xeobkHoGZWdco5wjWAE/0be8s9h1H0mZJ2yRt27Nnz2l9qaT8LmUOAjMzYLRBoAH7Bi4AFBFbI2IqIqYmJydP+4snvPCcmVnPKINgJ3Bp3/ZaYNdSfPF4I/MFZWZmhVEGwd3ArxRnD10FPBsRTy7FF080qhx0j8DMDIBqWR8s6Q7gVcBqSTuB3wZqABGxBbgXuA7YARwCbiyrLvP5ngRmZseUFgQRccMJjgfwrrK+fzETjSpPzxwaxVebmZ11kruyGPIg8FlDZma5JINg3EFgZtaTbBB4jsDMLJdkEKxaUWW2HRxt+RRSM7Mkg2C8nt+TwNcSmJmlGgS+b7GZWU+SQeCb05iZHZNkEPSWom46CMzMkgyCRjVvdrPVGXFNzMxGL8kgqHWDoO0gMDNLMgjqWd7sWfcIzMwSDYKiRzDbHnj7AzOzpCQZBLVuj8BDQ2ZmqQZBfnM0zxGYmSUaBHX3CMzMepIMgponi83MetIMAk8Wm5n1pBkEniMwM+tJMwgqniMwM+tKMggqFVGtyEFgZkaiQQD5hLHXGjIzSzoI5MliMzNKDgJJ10jaLmmHpJsHHD9f0p9K+ntJj0i6scz69KtXK54sNjOjxCCQlAG3ANcCG4EbJG2cV+xdwLcj4grgVcDvSqqXVad+tazi6wjMzCi3R3AlsCMiHo2IJnAnsGlemQBWSRIwATwNLMndYmpZxZPFZmaUGwRrgCf6tncW+/p9CvgJYBfwLeDdEXHcr7OkzZK2Sdq2Z8+eM1I5zxGYmeXKDAIN2Df/l/f1wDeB5wIvAT4l6bzj3hSxNSKmImJqcnLyjFSulnmOwMwMyg2CncClfdtryf/y73cjcFfkdgDfB368xDr1NKoeGjIzg3KD4AFgg6TLigng64G755X5AfAaAEk/BrwIeLTEOvV4jsDMLFct64MjoiXpJuA+IANui4hHJL2zOL4F+Ahwu6RvkQ8lvS8i9pZVp375WUOeIzAzKy0IACLiXuDeefu29L3eBbyuzDospFatcPjw7Ci+2szsrJLslcX1zGsNmZlBwkHgOQIzs1ziQeA5AjOzpIPAq4+amSUcBPWq5wjMzCDhIPCVxWZmuZMOAkmVQctAnGu8+qiZWW6oIJD0x5LOkzQOfBvYLuk3yq1auTxZbGaWG7ZHsDEiDgC/SH6B2DrgraXVagnUM9Fsd4hwGJhZ2oYNgpqkGnkQfDEiZjl+JdFzSi3Lm97qnNPNMDM7bcMGwX8HHgPGga9Keh5woKxKLYVaNW+6zxwys9QNFQQR8XsRsSYiriuWjH4cuLrkupWq2yPwwnNmlrphJ4vfXUwWS9LvS/oG8OqS61aqetEj8CmkZpa6YYeG/k0xWfw6YJL8hjIfLa1WS6Ce5TdQ89CQmaVu2CDo3nbyOuAPIuLvGXwrynNGb2jIQWBmiRs2CB6U9BXyILhP0irgnP4FdRCYmeWGvTHN28lvLv9oRBySdBH58NA5qxsETU8Wm1nihu0RvBzYHhH7Jb0F+CDwbHnVKl+96jkCMzMYPgg+AxySdAXwm8DjwGdLq9US8NCQmVlu2CBoRb4WwybgkxHxSWBVedUqX29oyEFgZokbdo7goKT3k68v9EpJGVArr1rlO9Yj8ByBmaVt2B7BG4Gj5NcTPAWsAT5+ojdJukbSdkk7JN28QJlXSfqmpEck/Z+ha36a6r3JYvcIzCxtwy4x8RTwR8D5kt4AHImIRecIil7DLcC1wEbgBkkb55W5APg08AsR8ZPAvz75JpyamieLzcyA4ZeY+GXg78h/qH8Z+LqkXzrB264EdkTEoxHRBO4kn2Po9ybgroj4AUBE7D6Zyp8OTxabmeWGnSP4APCy7g+1pEngz4HPL/KeNcATfds7gZ+ZV+aF5Etc/zX55PMnB/U0JG0GNgOsW7duyCovzkNDZma5YecIKvP+Wt83xHsHLUExf2a2Cvw08PPA64HfkvTC494UsTUipiJianJycsgqL86TxWZmuWF7BF+WdB9wR7H9RvI7lS1mJ3Bp3/ZaYNeAMnsjYgaYkfRV4Argu0PW65TVvOicmRkw/GTxbwBbgcvJf6i3RsT7TvC2B4ANki6TVAeuB+6eV+aL5KejViWNkQ8dfedkGnCqfGMaM7PcsD0CIuILwBdOonxL0k3AfUAG3BYRj0h6Z3F8S0R8R9KXgYfIF7G7NSIePqkWnKK6LygzMwNOEASSDjL43sQCIiLOW+z9EXEv84aQImLLvO2PM8Q1CWea71BmZpZbNAgi4pxeRmIxWUVkFXloyMySN+xZQ8tSLXMQmJklHgQVzxGYWfKSDoJ6VnGPwMySl3QQ1LKKJ4vNLHlpB0HVcwRmZmkHgecIzMzSDoJ6VvGic2aWvKSDoObJYjOz1INAXn3UzJKXeBB4jsDMLOkgqFc9NGRmlnQQeI7AzCz5IJAvKDOz5CUdBPVq5h6BmSUv6SCoZfJksZklL+kg8KJzZmaJB0E+Wew5AjNLm4PAS0yYWeLSDoKq5wjMzJIOAs8RmJmVHASSrpG0XdIOSTcvUu5lktqSfqnM+sxXyyp0AtodzxOYWbpKCwJJGXALcC2wEbhB0sYFyn0MuK+suiykluXNd6/AzFJWZo/gSmBHRDwaEU3gTmDTgHK/BnwB2F1iXQaqZQLgqCeMzSxhZQbBGuCJvu2dxb4eSWuAfwVsWeyDJG2WtE3Stj179pyxCtar7hGYmZUZBBqwb/5g/CeA90VEe7EPioitETEVEVOTk5NnrIIeGjIzg2qJn70TuLRvey2wa16ZKeBOSQCrgesktSLiT0qsV08vCLzwnJklrMwgeADYIOky4IfA9cCb+gtExGXd15JuB+5ZqhCAY3MEvpbAzFJWWhBEREvSTeRnA2XAbRHxiKR3FscXnRdYCnUPDZmZldojICLuBe6dt29gAETE28qsyyCeIzAzS/zK4prPGjIzSzsIukNDTU8Wm1nC0g6Caj5Z7B6BmaUs6SDwHIGZmYMAcBCYWdocBEDTdykzs4QlHQS96wi86JyZJSzpIKh5stjMLPEg8ByBmZmDAHw/AjNLW9JBcGytIU8Wm1m6kg6C7uqjHhoys5QlHQRZRUgOAjNLW9JBIIlaVvH9CMwsaUkHAeTzBL5DmZmlLPkgqGXy0JCZJc1BkFUcBGaWtOSDoF71HIGZpc1BkFV8HYGZJS35IKhlFS86Z2ZJcxBUPVlsZmkrNQgkXSNpu6Qdkm4ecPzNkh4qHl+TdEWZ9RnE1xGYWepKCwJJGXALcC2wEbhB0sZ5xb4P/FxEXA58BNhaVn0W4rOGzCx1ZfYIrgR2RMSjEdEE7gQ29ReIiK9FxDPF5v3A2hLrM5Ani80sdWUGwRrgib7tncW+hbwd+FKJ9RnIF5SZWeqqJX62Buwb+Ke3pKvJg+AVCxzfDGwGWLdu3ZmqH1DMEfisITNLWJk9gp3ApX3ba4Fd8wtJuhy4FdgUEfsGfVBEbI2IqYiYmpycPKOVrPmCMjNLXJlB8ACwQdJlkurA9cDd/QUkrQPuAt4aEd8tsS4Lqnuy2MwSV9rQUES0JN0E3AdkwG0R8YikdxbHtwAfAi4GPi0JoBURU2XVaZBaJq8+amZJK3OOgIi4F7h33r4tfa/fAbyjzDqciE8fNbPU+cpiX1BmZolLPgjqVfcIzCxtyQdBfh2B5wjMLF3JB0E9y2h3gnbHYWBmaUo+CGrV/Lo3Dw+ZWaqSD4J6lv8jcBCYWaqSD4JaLwg8NGRmaXIQuEdgZolzEGT5HIEXnjOzVCUfBPWqewRmlrbkg8BzBGaWOgdBEQQeGjKzVDkIunMEHhoys0QlHwS+jsDMUpd8ENQ8WWxmiSv1fgTngu4cwY7d01xy/krG6hkTK6pM1KtUKoNuu2xmtrwkHwQXrKwB8OE//fZxxyYaVc5bUeW8lTXOW1njgpU1Lhyrc8F4jYvG6lw0fuxx8XiDiyfqjNUzirutmZmdE5IPgvWrx7nn117Bjw4cYabZ5tDRFtNHWxw40uLgkVkOHG7x7OFZDhye5bF9M3zzif3sPzS74OTyilqFi8cbrJ6oc/FE/rx6otF7PTnRYHJVg9UTDc5fWXOvw8xGLvkgAHjxmvN58Zrzhy4fEcw02zwz02TfTJNnZprsnT7KvpkmT3dfTzf50YEjPLLrWfZNN2kNWOa6WhEXF0HRe6zKw6J//8UTdS4aq1PNkp/SMbMSOAhOgSQmGlUmGlUuvWjshOUjgmcPz7J3+ii7Dx5l73STvQePsne6+8jD43s/Osje6ebA3oYEF47Vubg7FDWRD0d1X3ePXTieP18wVu9dNW1mthgHwRKQxAVj+Y/zC56zatGyEcGBI61er6IbFvumm+yb6T432f7UQfbN7GP/odkFP2uiUeXC8WJeY6zOhWP56/NX1rhgrDbn+fxiHuT8lTUa1exM/yMws7OYg+AsI6n3w/xPJk9cvtXu8MyhWZ4uhqWeOZQHxf6ZJk8fyoetnjk0y/5DTR7bO8P+Q00OHGkt+pmNaoXzVtZYtaLKeSuOPZ+3slr0hPJ9E40qEyuqjDeqvR7SeCNjvF5lrJFRzyqeODc7B5QaBJKuAT4JZMCtEfHRecdVHL8OOAS8LSK+UWadlptqVmFyVT4BPax2JzhweJb9h2d59nAeEs8enuXAkRYHin3difIDR/L9P9x/mINHWkwfaXF4tj1c3SpirJ4x3qiysp4xVs8Yq+WvV9by7RXF65W1jJX1jEa1wopaVjwqrKhmNGoVGtV8u1HNyzRqFepZhUYtD5xaJoeO2SkqLQgkZcAtwL8AdgIPSLo7IvrP07wW2FA8fgb4TPFsJcoq4sJiPuFUtNodpouzq6aP5uEwfbTFoWab6aMtZvpeH262OdRsMdNs917vP9Tkydk2h4p9R2bbHJ5tc7q3ja5nFerV4pFVqFVFLSteF2FR63tdLY5VM1Gt5PuySl6mWhFZJmqVCllFVCt5+awCWSU/Xin2Z8rf1/+o9PZx7LXysOrul/J9FYlKUS4vmx/Lt/P90rHjFeXHu/tE8VwB0X0viLxMfzn1nnFwWk+ZPYIrgR0R8SiApDuBTUB/EGwCPhsRAdwv6QJJl0TEkyXWy05TNav05jzOlIig2e5wZLbD0SIYjrY6HJ3tcKSVh0Wz1eFoq9N73Wznx5vtfH+z+2jnx2fb+Wc2Wx1a7WPbh5otZtvBbLtDqxO9Y61Oh1axv90JZotjpxtQZzvp+ACht68o0w0VjgWIiv/pBUyv7Nwyc/NGc77z2Ou5odT/Hs3ZPzi85pSf814tsH+w/s9fNCaHyNBhYvZkw/j6l13KO175/JN6zzDKDII1wBN92zs5/q/9QWXWAHOCQNJmYDPAunXrznhFbfQkFcM+GRQX+Z0tIqIIjKAdQbsIiHYEnQ60OnlwtDtBpyjb7uTHuuU7fc/d/Z0IIoJ2h+Ned3rHoRPQ6QRB0Il8aC+KenV6r+mV75brvo7e++fui7xxx+3vFK8pArD7XXFs15zy3X9Gx8p233fsePcYveNx7HPmBW1wbMeg9x+/f3ChueVj4P4537vAe48vd+K/DIb62+EU/sBYPTH8EPDJKDMIBkXd/KYPU4aI2ApsBZiamlrmf5/Z2UZSMaw06pqYlaPME813Apf2ba8Fdp1CGTMzK1GZQfAAsEHSZZLqwPXA3fPK3A38inJXAc96fsDMbGmVNjQUES1JNwH3kZ8+eltEPCLpncXxLcC95KeO7iA/ffTGsupjZmaDlXodQUTcS/5j379vS9/rAN5VZh3MzGxxXozGzCxxDgIzs8Q5CMzMEucgMDNLnIa5Su5sImkP8Pgpvn01sPcMVudckWK7U2wzpNnuFNsMJ9/u50XEwDWNz7kgOB2StkXE1KjrsdRSbHeKbYY0251im+HMtttDQ2ZmiXMQmJklLrUg2DrqCoxIiu1Osc2QZrtTbDOcwXYnNUdgZmbHS61HYGZm8zgIzMwSl0wQSLpG0nZJOyTdPOr6lEHSpZL+StJ3JD0i6d3F/osk/Zmk7xXPF466rmeapEzS/5N0T7GdQpsvkPR5Sf9Q/Dt/eSLt/vfFf98PS7pD0orl1m5Jt0naLenhvn0LtlHS+4vftu2SXn+y35dEEEjKgFuAa4GNwA2SNo62VqVoAe+NiJ8ArgLeVbTzZuAvImID8BfF9nLzbuA7fdsptPmTwJcj4seBK8jbv6zbLWkN8O+AqYh4MfkS99ez/Np9O3DNvH0D21j8f/x64CeL93y6+M0bWhJBAFwJ7IiIRyOiCdwJbBpxnc64iHgyIr5RvD5I/sOwhrytf1gU+0PgF0dTw3JIWgv8PHBr3+7l3ubzgH8O/D5ARDQjYj/LvN2FKrBSUhUYI7+r4bJqd0R8FXh63u6F2rgJuDMijkbE98nv73LlyXxfKkGwBniib3tnsW/ZkrQe+Cng68CPde/8Vjw/Z3Q1K8UngN8EOn37lnubnw/sAf6gGBK7VdI4y7zdEfFD4HeAHwBPkt/V8Css83YXFmrjaf++pRIEGrBv2Z43K2kC+ALwnog4MOr6lEnSG4DdEfHgqOuyxKrAS4HPRMRPATOc+8MhJ1SMi28CLgOeC4xLestoazVyp/37lkoQ7AQu7dteS96dXHYk1chD4I8i4q5i948kXVIcvwTYPar6leBngV+Q9Bj5kN+rJX2O5d1myP+b3hkRXy+2P08eDMu93a8Fvh8ReyJiFrgL+Gcs/3bDwm087d+3VILgAWCDpMsk1cknVu4ecZ3OOEkiHzP+TkT8175DdwO/Wrz+VeCLS123skTE+yNibUSsJ//3+pcR8RaWcZsBIuIp4AlJLyp2vQb4Nsu83eRDQldJGiv+e38N+VzYcm83LNzGu4HrJTUkXQZsAP7upD45IpJ4ANcB3wX+EfjAqOtTUhtfQd4lfAj4ZvG4DriY/CyD7xXPF426riW1/1XAPcXrZd9m4CXAtuLf958AFybS7g8D/wA8DPwPoLHc2g3cQT4HMkv+F//bF2sj8IHit207cO3Jfp+XmDAzS1wqQ0NmZrYAB4GZWeIcBGZmiXMQmJklzkFgZpY4B4HZKZD0Hkljo66H2Zng00fNTkFxJfNUROwddV3MTld11BUwO9sVi7n9L/JL9zPgf5Ovc/NXkvZGxNWSXkd+oVOD/MKeGyNiugiM/wlcXXzcmyJix1K3wWwxHhoyO7FrgF0RcUXka+B/gnwtl6uLEFgNfBB4bUS8lPxq31/ve/+BiLgS+FTxXrOzioPA7MS+BbxW0sckvTIinp13/CryGx79X0nfJF8H5nl9x+/oe3556bU1O0keGjI7gYj4rqSfJl+36b9I+sq8IgL+LCJuWOgjFnhtdlZwj8DsBCQ9FzgUEZ8jvynKS4GDwKqiyP3Az0p6QVF+TNIL+z7ijX3Pf7s0tTYbnnsEZif2T4GPS+qQrwb5b8mHeL4k6clinuBtwB2SGsV7Pki+2i1AQ9LXyf/wWqjXYDYyPn3UrEQ+zdTOBR4aMjNLnHsEZmaJc4/AzCxxDgIzs8Q5CMzMEucgMDNLnIPAzCxx/x/B4wNCM/uNhQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title('loss')\n",
    "plt.plot(np.arange(len(v_losses)), v_losses)\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('losss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
